{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction, much of EDA are done elsewhere.  Only minimmal checks.\n",
        "  > pls run under r environment\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import and setup\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "\n",
        "# By proximity to Informatics Forum\n",
        "# 253  -> 15726  -> 19260\n",
        "# BOTA -> HILVIW -> GOGAR\n",
        "BOTANIC  = 253;   # messy, missing\n",
        "HILLVIEW = 15726; # bad data\n",
        "GOGAR    = 19260; # good, more complete\n",
        "\n",
        "\n",
        "HEADERS_WH = pd.read_csv('../data/MIDAS/HEADERS/WH_Column_Headers.txt').columns.to_numpy();\n",
        "HEADERS_WH = np.array( list( map( lambda str: str.strip(), HEADERS_WH) ) )\n",
        "\n",
        "HEADERS_RH = pd.read_csv('../data/MIDAS/HEADERS/RH_Column_Headers.csv').columns.to_numpy();\n",
        "HEADERS_RH = np.array( list( map( lambda str: str.strip(), HEADERS_RH) ) )\n",
        "\n",
        "HEADERS_WH\n",
        "HEADERS_RH\n",
        "\n",
        "# WH_2013 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201301-201312.txt', header=None, index_col=False);\n",
        "# WH_2014 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201401-201412.txt', header=None, index_col=False);\n",
        "# WH_2015 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201501-201512.txt', header=None, index_col=False);\n",
        "# WH_2016 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201601-201612.txt', header=None, index_col=False);\n",
        "# WH_2017 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201701-201712.txt', header=None, index_col=False);\n",
        "# WH_2018 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201801-201812.txt', header=None, index_col=False);\n",
        "# WH_2019 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_201901-201912.txt', header=None, index_col=False);\n",
        "# WH_2020 = pd.read_csv('../data/MIDAS/WH/midas_wxhrly_202001-202012.txt', header=None, index_col=False);\n",
        "#\n",
        "# RH_2013 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201301-201312.txt', header=None, index_col=False);\n",
        "# RH_2014 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201401-201412.txt', header=None, index_col=False);\n",
        "# RH_2015 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201501-201512.txt', header=None, index_col=False);\n",
        "# RH_2016 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201601-201612.txt', header=None, index_col=False);\n",
        "# RH_2017 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201701-201712.txt', header=None, index_col=False);\n",
        "# RH_2018 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201801-201812.txt', header=None, index_col=False);\n",
        "# RH_2019 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_201901-201912.txt', header=None, index_col=False);\n",
        "# RH_2020 = pd.read_csv('../data/MIDAS/RH/sed/midas_rainhrly_202001-202012.txt', header=None, index_col=False);\n",
        "\n",
        "\n",
        "\n",
        "# WH_2013.columns  = HEADERS_WH;\n",
        "# WH_2014.columns = HEADERS_WH;\n",
        "# WH_2015.columns = HEADERS_WH;\n",
        "# WH_2016.columns = HEADERS_WH;\n",
        "# WH_2017.columns = HEADERS_WH;\n",
        "# WH_2018.columns = HEADERS_WH;\n",
        "# WH_2019.columns = HEADERS_WH;\n",
        "# WH_2020.columns = HEADERS_WH;\n",
        "#\n",
        "# RH_2013.columns = HEADERS_RH;\n",
        "# RH_2014.columns = HEADERS_RH;\n",
        "# RH_2015.columns = HEADERS_RH;\n",
        "# RH_2016.columns = HEADERS_RH;\n",
        "# RH_2017.columns = HEADERS_RH;\n",
        "# RH_2018.columns = HEADERS_RH;\n",
        "# RH_2019.columns = HEADERS_RH;\n",
        "# RH_2020.columns = HEADERS_RH;\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTES\n",
        "expected number of rows:\n",
        "\n",
        "**2012, 2016, 2020**\n",
        "- 366 * 24 = 8784;\n",
        "\n",
        "**2013, 2014, 2015, 2017, 2018, 2019 **\n",
        "- 365 * 24 = 8760;\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # These THREE (3) should hv 8760 rows\n",
        "# WH_2013_out = WH_2013[ np.logical_and( WH_2013['SRC_ID'] == GOGAR, WH_2013['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2013_out.shape #TRUE\n",
        "# WH_2013_out['OB_TIME'] = pd.to_datetime(WH_2013_out['OB_TIME'])\n",
        "# WH_2013_out = WH_2013_out.set_index('OB_TIME')\n",
        "# print(WH_2013_out.index.min(), WH_2013_out.index.max())\n",
        "# WH_2013_out = WH_2013_out.reindex(pd.date_range(WH_2013_out.index.min(), WH_2013_out.index.max(), freq='H'))  # ASSUME min and max are correct;\n",
        "# WH_2013_out = WH_2013_out.index.to_frame(name='OB_TIME').join(WH_2013_out).reset_index(drop=True)\n",
        "# WH_2013_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2013.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# WH_2014_out = WH_2014[ np.logical_and( WH_2014['SRC_ID'] == GOGAR, WH_2014['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2014_out.shape #FALSE, miss 8\n",
        "# WH_2014_out['OB_TIME'] = pd.to_datetime(WH_2014_out['OB_TIME'])\n",
        "# WH_2014_out = WH_2014_out.set_index('OB_TIME')\n",
        "# print(WH_2014_out.index.min(), WH_2014_out.index.max())\n",
        "# WH_2014_out = WH_2014_out.reindex(pd.date_range(WH_2014_out.index.min(), WH_2014_out.index.max(), freq='H'))\n",
        "# WH_2014_out = WH_2014_out.index.to_frame(name='OB_TIME').join(WH_2014_out).reset_index(drop=True)\n",
        "# WH_2014_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2014.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# WH_2015_out = WH_2015[ np.logical_and( WH_2015['SRC_ID'] == GOGAR, WH_2015['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2015_out.shape #FALSE, miss 2\n",
        "# WH_2015_out['OB_TIME'] = pd.to_datetime(WH_2015_out['OB_TIME'])\n",
        "# WH_2015_out = WH_2015_out.set_index('OB_TIME')\n",
        "# print(WH_2015_out.index.min(), WH_2015_out.index.max())\n",
        "# WH_2015_out = WH_2015_out.reindex(pd.date_range(WH_2015_out.index.min(), WH_2015_out.index.max(), freq='H'))\n",
        "# WH_2015_out = WH_2015_out.index.to_frame(name='OB_TIME').join(WH_2015_out).reset_index(drop=True)\n",
        "# WH_2015_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2015.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "# # 2016 is leap year, should hv 8784 rows\n",
        "#\n",
        "# WH_2016_out = WH_2016[ np.logical_and( WH_2016['SRC_ID'] == GOGAR, WH_2016['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2016_out.shape #TRUE\n",
        "# WH_2016_out['OB_TIME'] = pd.to_datetime(WH_2016_out['OB_TIME'])\n",
        "# WH_2016_out = WH_2016_out.set_index('OB_TIME')\n",
        "# print(WH_2016_out.index.min(), WH_2016_out.index.max())\n",
        "# WH_2016_out = WH_2016_out.reindex(pd.date_range(WH_2016_out.index.min(), WH_2016_out.index.max(), freq='H'))\n",
        "# WH_2016_out = WH_2016_out.index.to_frame(name='OB_TIME').join(WH_2016_out).reset_index(drop=True)\n",
        "# WH_2016_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2016.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "# # These THREE (3) should hv 8760 rows\n",
        "#\n",
        "# WH_2017_out = WH_2017[ np.logical_and( WH_2017['SRC_ID'] == GOGAR, WH_2017['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2017_out.shape #FALSE 4\n",
        "# WH_2017_out['OB_TIME'] = pd.to_datetime(WH_2017_out['OB_TIME'])\n",
        "# WH_2017_out = WH_2017_out.set_index('OB_TIME')\n",
        "# print(WH_2017_out.index.min(), WH_2017_out.index.max())\n",
        "# WH_2017_out = WH_2017_out.reindex(pd.date_range(WH_2017_out.index.min(), WH_2017_out.index.max(), freq='H'))\n",
        "# WH_2017_out = WH_2017_out.index.to_frame(name='OB_TIME').join(WH_2017_out).reset_index(drop=True)\n",
        "# WH_2017_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2017.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "# WH_2018_out = WH_2018[ np.logical_and( WH_2018['SRC_ID'] == GOGAR, WH_2018['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2018_out.shape #FALSE 46\n",
        "# WH_2018_out['OB_TIME'] = pd.to_datetime(WH_2018_out['OB_TIME'])\n",
        "# WH_2018_out = WH_2018_out.set_index('OB_TIME')\n",
        "# print(WH_2018_out.index.min(), WH_2018_out.index.max())\n",
        "# WH_2018_out = WH_2018_out.reindex(pd.date_range(WH_2018_out.index.min(), WH_2018_out.index.max(), freq='H'))\n",
        "# WH_2018_out = WH_2018_out.index.to_frame(name='OB_TIME').join(WH_2018_out).reset_index(drop=True)\n",
        "# WH_2018_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2018.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "# WH_2019_out = WH_2019[ np.logical_and( WH_2019['SRC_ID'] == GOGAR, WH_2019['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2019_out.shape #TRUE\n",
        "# WH_2019_out['OB_TIME'] = pd.to_datetime(WH_2019_out['OB_TIME'])\n",
        "# WH_2019_out = WH_2019_out.set_index('OB_TIME')\n",
        "# print(WH_2019_out.index.min(), WH_2019_out.index.max())\n",
        "# WH_2019_out = WH_2019_out.reindex(pd.date_range(WH_2019_out.index.min(), WH_2019_out.index.max(), freq='H'))\n",
        "# WH_2019_out = WH_2019_out.index.to_frame(name='OB_TIME').join(WH_2019_out).reset_index(drop=True)\n",
        "# WH_2019_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2019.csv', index=False);\n",
        "#\n",
        "#\n",
        "#\n",
        "# WH_2020_out = WH_2020[ np.logical_and( WH_2020['SRC_ID'] == GOGAR, WH_2020['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_TIME'], keep='last');\n",
        "# WH_2020_out.shape #TRUE ( beginning of JAN to end of MAY )\n",
        "# WH_2020_out['OB_TIME'] = pd.to_datetime(WH_2020_out['OB_TIME'])\n",
        "# WH_2020_out = WH_2020_out.set_index('OB_TIME')\n",
        "# print(WH_2020_out.index.min(), WH_2020_out.index.max())\n",
        "# WH_2020_out = WH_2020_out.reindex(pd.date_range(WH_2020_out.index.min(), WH_2020_out.index.max(), freq='H'))\n",
        "# WH_2020_out = WH_2020_out.index.to_frame(name='OB_TIME').join(WH_2020_out).reset_index(drop=True)\n",
        "# WH_2020_out.to_csv(path_or_buf='../data/preprocessed/WH/gogar_2020.csv', index=False);\n",
        "\n",
        "\n",
        "\n",
        "# RH_2013_out = RH_2013[ np.logical_and( RH_2013['SRC_ID'] == GOGAR, RH_2013['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2013_out = RH_2013_out[RH_2013_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2013_out.shape #FALSE 730\n",
        "# RH_2013_out['OB_END_TIME'] = pd.to_datetime(RH_2013_out['OB_END_TIME'])\n",
        "# RH_2013_out = RH_2013_out.set_index('OB_END_TIME')\n",
        "# print(RH_2013_out.index.min(), RH_2013_out.index.max())\n",
        "# RH_2013_out = RH_2013_out.reindex(pd.date_range(RH_2013_out.index.min(), RH_2013_out.index.max(), freq='H'))\n",
        "# RH_2013_out = RH_2013_out.index.to_frame(name='OB_END_TIME').join(RH_2013_out).reset_index(drop=True)\n",
        "# RH_2013_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2013.csv', index=False);\n",
        "\n",
        "\n",
        "# RH_2014_out = RH_2014[ np.logical_and( RH_2014['SRC_ID'] == GOGAR, RH_2014['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2014_out = RH_2014_out[RH_2014_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2014_out.shape #FALSE 737\n",
        "# RH_2014_out['OB_END_TIME'] = pd.to_datetime(RH_2014_out['OB_END_TIME'])\n",
        "# RH_2014_out = RH_2014_out.set_index('OB_END_TIME')\n",
        "# print(RH_2014_out.index.min(), RH_2014_out.index.max())\n",
        "# RH_2014_out = RH_2014_out.reindex(pd.date_range(RH_2014_out.index.min(), RH_2014_out.index.max(), freq='H'))\n",
        "# RH_2014_out = RH_2014_out.index.to_frame(name='OB_END_TIME').join(RH_2014_out).reset_index(drop=True)\n",
        "# RH_2014_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2014.csv', index=False);\n",
        "\n",
        "\n",
        "\n",
        "# RH_2015_out = RH_2015[ np.logical_and( RH_2015['SRC_ID'] == GOGAR, RH_2015['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2015_out = RH_2015_out[RH_2015_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2015_out.shape #FALSE 732\n",
        "# RH_2015_out['OB_END_TIME'] = pd.to_datetime(RH_2015_out['OB_END_TIME'])\n",
        "# RH_2015_out = RH_2015_out.set_index('OB_END_TIME')\n",
        "# print(RH_2015_out.index.min(), RH_2015_out.index.max())\n",
        "# RH_2015_out = RH_2015_out.reindex(pd.date_range(RH_2015_out.index.min(), RH_2015_out.index.max(), freq='H'))\n",
        "# RH_2015_out = RH_2015_out.index.to_frame(name='OB_END_TIME').join(RH_2015_out).reset_index(drop=True)\n",
        "# RH_2015_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2015.csv', index=False);\n",
        "\n",
        "\n",
        "# RH_2016_out = RH_2016[ np.logical_and( RH_2016['SRC_ID'] == GOGAR, RH_2016['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2016_out = RH_2016_out[RH_2016_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2016_out.shape #FALSE 732\n",
        "# RH_2016_out['OB_END_TIME'] = pd.to_datetime(RH_2016_out['OB_END_TIME'])\n",
        "# RH_2016_out = RH_2016_out.set_index('OB_END_TIME')\n",
        "# print(RH_2016_out.index.min(), RH_2016_out.index.max())\n",
        "# RH_2016_out = RH_2016_out.reindex(pd.date_range(RH_2016_out.index.min(), RH_2016_out.index.max(), freq='H'))\n",
        "# RH_2016_out = RH_2016_out.index.to_frame(name='OB_END_TIME').join(RH_2016_out).reset_index(drop=True)\n",
        "# RH_2016_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2016.csv', index=False);\n",
        "\n",
        "\n",
        "# RH_2017_out = RH_2017[ np.logical_and( RH_2017['SRC_ID'] == GOGAR, RH_2017['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2017_out = RH_2017_out[RH_2017_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2017_out.shape #FALSE 734\n",
        "# RH_2017_out['OB_END_TIME'] = pd.to_datetime(RH_2017_out['OB_END_TIME'])\n",
        "# RH_2017_out = RH_2017_out.set_index('OB_END_TIME')\n",
        "# print(RH_2017_out.index.min(), RH_2017_out.index.max())\n",
        "# RH_2017_out = RH_2017_out.reindex(pd.date_range(RH_2017_out.index.min(), RH_2017_out.index.max(), freq='H'))\n",
        "# RH_2017_out = RH_2017_out.index.to_frame(name='OB_END_TIME').join(RH_2017_out).reset_index(drop=True)\n",
        "# RH_2017_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2017.csv', index=False);\n",
        "\n",
        "\n",
        "# RH_2018_out = RH_2018[ np.logical_and( RH_2018['SRC_ID'] == GOGAR, RH_2018['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2018_out = RH_2018_out[RH_2018_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2018_out.shape #FALSE 771\n",
        "# RH_2018_out['OB_END_TIME'] = pd.to_datetime(RH_2018_out['OB_END_TIME'])\n",
        "# RH_2018_out = RH_2018_out.set_index('OB_END_TIME')\n",
        "# print(RH_2018_out.index.min(), RH_2018_out.index.max())\n",
        "# RH_2018_out = RH_2018_out.reindex(pd.date_range(RH_2018_out.index.min(), RH_2018_out.index.max(), freq='H'))\n",
        "# RH_2018_out = RH_2018_out.index.to_frame(name='OB_END_TIME').join(RH_2018_out).reset_index(drop=True)\n",
        "# RH_2018_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2018.csv', index=False);\n",
        "\n",
        "\n",
        "# RH_2019_out = RH_2019[ np.logical_and( RH_2019['SRC_ID'] == GOGAR, RH_2019['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2019_out = RH_2019_out[RH_2019_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2019_out.shape #FALSE 730\n",
        "# RH_2019_out['OB_END_TIME'] = pd.to_datetime(RH_2019_out['OB_END_TIME'])\n",
        "# RH_2019_out = RH_2019_out.set_index('OB_END_TIME')\n",
        "# print(RH_2019_out.index.min(), RH_2019_out.index.max())\n",
        "# RH_2019_out = RH_2019_out.reindex(pd.date_range(RH_2019_out.index.min(), RH_2019_out.index.max(), freq='H'))\n",
        "# RH_2019_out = RH_2019_out.index.to_frame(name='OB_END_TIME').join(RH_2019_out).reset_index(drop=True)\n",
        "# RH_2019_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2019.csv', index=False);\n",
        "\n",
        "\n",
        "\n",
        "# RH_2020_out = RH_2020[ np.logical_and( RH_2020['SRC_ID'] == GOGAR, RH_2020['VERSION_NUM'] == 1) ].drop_duplicates(subset=['OB_END_TIME'], keep='last');\n",
        "# RH_2020_out = RH_2020_out[RH_2020_out['OB_HOUR_COUNT'] == 1]; #hourly data only\n",
        "# RH_2020_out.shape #FALSE unknown\n",
        "# RH_2020_out['OB_END_TIME'] = pd.to_datetime(RH_2020_out['OB_END_TIME'])\n",
        "# RH_2020_out = RH_2020_out.set_index('OB_END_TIME')\n",
        "# print(RH_2020_out.index.min(), RH_2020_out.index.max())\n",
        "# RH_2020_out = RH_2020_out.reindex(pd.date_range(RH_2020_out.index.min(), RH_2020_out.index.max(), freq='H'))\n",
        "# RH_2020_out = RH_2020_out.index.to_frame(name='OB_END_TIME').join(RH_2020_out).reset_index(drop=True)\n",
        "# RH_2020_out.to_csv(path_or_buf='../data/preprocessed/RH/gogar_rain_2020.csv', index=False);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "/Users/lautinyeung/miniconda3/envs/r/bin/python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}